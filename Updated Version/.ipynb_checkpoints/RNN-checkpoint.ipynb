{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import glob\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "#All the MIDIs have been converted into CSVs to be quickly read into the RNN are all stored in this folder\n",
    "def getMidiCSV():\n",
    "    return glob.glob(\"Midi-CSVs/*.csv\")\n",
    "\n",
    "#For the creation of the label array for the RNN\n",
    "def get_label_vector(label):\n",
    "    #Stores the correct label of the midi file by coverting the index value of at the corresponding genre to a 1\n",
    "    #Is a vector for batch purposes\n",
    "    label_array = np.zeros((7))\n",
    "    label_dict = {'hh':0,\n",
    "                 'cl':1,\n",
    "                 'cn':2,\n",
    "                 'ro':3,\n",
    "                 'ed':4,\n",
    "                 'pp':5,\n",
    "                 'mt':6}\n",
    "    label_array[label_dict[label]] = 1\n",
    "    return label_array\n",
    "\n",
    "def generateMidiData():\n",
    "    #Finds the file names of all the MIDI CSVs and stored them into a list\n",
    "    MidiCSVs = getMidiCSV()\n",
    "    \n",
    "    #Must have at least one midi to run program and since all are same size open first to see the size of the numpy\n",
    "    sizeCheck = np.loadtxt(MidiCSVs[0], delimiter=',')\n",
    "    \n",
    "    #Store size to pass to midi_note for RNN\n",
    "    samples = sizeCheck.shape[0]\n",
    "    \n",
    "    #Generates numpy arrays to hold all note matrices and corresponding labels\n",
    "    midi_note = np.zeros(shape=(len(MidiCSVs),samples,128))\n",
    "    midi_label = np.zeros(shape=(len(MidiCSVs),7))\n",
    "\n",
    "    for i in range(len(MidiCSVs)):\n",
    "        csv = MidiCSVs[i]\n",
    "        #Loads matrix at i-th position into numpy array\n",
    "        noteMatrix = np.loadtxt(csv, delimiter=',')\n",
    "        \n",
    "        #convert index value of midi matrix with the current notematrix\n",
    "        midi_note[i] = noteMatrix\n",
    "\n",
    "        #stores the label of the midi file which is the first two letters of each midi after the path has been removed\n",
    "        abbrev = csv.split(\"\\\\\")[1][:2]\n",
    "        midi_label[i] = get_label_vector(abbrev)\n",
    "    \n",
    "    return midi_note, midi_label, samples\n",
    "\n",
    "def train_test_split(raw_data_x, raw_data_y, decimal_split):\n",
    "    #Creates a randoms set of numbers of the size data to randomize the raw data\n",
    "    length = raw_data_x.shape[0]\n",
    "    permutation = np.random.permutation(length)\n",
    "    \n",
    "    #Randomizes the data but makes sure that the x'values and the y'classifications stay correlated\n",
    "    shuffled_raw_data_x = raw_data_x[permutation]\n",
    "    shuffled_raw_data_y = raw_data_y[permutation]\n",
    "    \n",
    "    #Finds the length of the split\n",
    "    idx = int(length * decimal_split)\n",
    "    \n",
    "    #Cuts the training, testing slices from the randomly shuffled sets\n",
    "    train_x = shuffled_raw_data_x[:idx]\n",
    "    test_x = shuffled_raw_data_x[idx:]\n",
    "    train_y = shuffled_raw_data_y[:idx]\n",
    "    test_y = shuffled_raw_data_y[idx:]\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def RNN(x, weights, biases, timesteps, num_hidden):\n",
    "\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.LSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    #Dropout layer\n",
    "    lstm_cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=0.9) #0.9\n",
    "    \n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #Splits the data randomly into testing and training\n",
    "    midi_note, midi_label, samples = generateMidiData()\n",
    "    decimal_split = 0.85\n",
    "    train_x, test_x, train_y, test_y = train_test_split(midi_note, midi_label, decimal_split)\n",
    "    \n",
    "    # Training Parameters\n",
    "    learning_rate = 0.002 #0.002\n",
    "    training_steps = 400 #400\n",
    "    batch_size = train_x.shape[0]\n",
    "    display_step = 10\n",
    "\n",
    "    # Network Parameters\n",
    "    num_input = 128 #instruments\n",
    "    timesteps = samples # timesteps\n",
    "    num_hidden = 18 #hidden layer num of features #18\n",
    "    num_classes = 7 #Total amount of genres\n",
    "    \n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "    Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "    \n",
    "    # Define weights and bias as normalized gaussian values to prevent issues from initializing as 0\n",
    "    weights = {\n",
    "        'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "    \n",
    "    #Output of the RNN\n",
    "    logits = RNN(X, weights, biases, timesteps, num_hidden)\n",
    "\n",
    "    #Prediction made from the softmax output of the RNN\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    #Loss is the softman cross entropy as we are doing prediction and with logits as using outputs from RNN\n",
    "    #Optimizer was ADAM as did best\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    #Gradient clipping\n",
    "    #Deals with the exploding gradient problem\n",
    "    gradients, variables = zip(*optimizer.compute_gradients(loss_op))\n",
    "\n",
    "    #Clips it by the maximum L2 norm\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0) #1.0\n",
    "\n",
    "    #Apply the gradients to our optimizer\n",
    "    train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    # Evaluate model by taking the highest prediction and setting to 1\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "        \n",
    "    # Start training\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        #Runs for the defined epochs\n",
    "        for step in range(1, training_steps+1):\n",
    "            #Over all the data in the batch sizes\n",
    "            for i in range(0, train_x.shape[0], batch_size):\n",
    "                batch_x = train_x[i:i+batch_size]\n",
    "                batch_y = train_y[i:i+batch_size]\n",
    "                # Run optimization op (backprop)\n",
    "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "                if step % display_step == 0 or step == 1:\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                         Y: batch_y})\n",
    "                    print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        # Calculate accuracy for test over batch sizes also\n",
    "        for i in range(0, test_x.shape[0], batch_size):\n",
    "            testing_data = test_x[i:i+batch_size]\n",
    "            testing_label = test_y[i:i+batch_size]\n",
    "            print(\"Testing Accuracy:\", \\\n",
    "                sess.run(accuracy, feed_dict={X: testing_data, Y: testing_label}))    \n",
    "        \n",
    "    sess.close()\n",
    "        \n",
    "if __name__== \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
